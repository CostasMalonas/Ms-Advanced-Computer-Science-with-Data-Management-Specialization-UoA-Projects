{"cells":[{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-10-30T17:01:06.667544Z","iopub.status.busy":"2023-10-30T17:01:06.667232Z","iopub.status.idle":"2023-10-30T17:01:06.672990Z","shell.execute_reply":"2023-10-30T17:01:06.671828Z","shell.execute_reply.started":"2023-10-30T17:01:06.667522Z"}},"source":["#### <h1>Φόρτωση βιβλιοθηκών</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T14:49:16.976140Z","iopub.status.busy":"2023-11-21T14:49:16.975638Z","iopub.status.idle":"2023-11-21T14:49:25.227517Z","shell.execute_reply":"2023-11-21T14:49:25.226275Z","shell.execute_reply.started":"2023-11-21T14:49:16.976095Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import re\n","import json\n","import spacy\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from wordcloud import WordCloud\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n","import numpy as np\n","from sklearn.model_selection import train_test_split,StratifiedKFold\n","import itertools\n","from sklearn.preprocessing import StandardScaler, LabelEncoder"]},{"cell_type":"markdown","metadata":{},"source":["## <h1>Φόρτωση των δεδομένων μας</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-11-21T14:49:25.231783Z","iopub.status.busy":"2023-11-21T14:49:25.230830Z","iopub.status.idle":"2023-11-21T14:49:25.832173Z","shell.execute_reply":"2023-11-21T14:49:25.830881Z","shell.execute_reply.started":"2023-11-21T14:49:25.231722Z"},"trusted":true},"outputs":[],"source":["df_train_set = pd.read_csv('/kaggle/input/ys19-2023-assignment-1/train_set.csv')\n","df_test_set = pd.read_csv('/kaggle/input/ys19-2023-assignment-1/test_set.csv')\n","df_valid_set = pd.read_csv('/kaggle/input/ys19-2023-assignment-1/valid_set.csv')"]},{"cell_type":"markdown","metadata":{},"source":["**Let's check for null values**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T14:49:25.834031Z","iopub.status.busy":"2023-11-21T14:49:25.833652Z","iopub.status.idle":"2023-11-21T14:49:25.919546Z","shell.execute_reply":"2023-11-21T14:49:25.917926Z","shell.execute_reply.started":"2023-11-21T14:49:25.833998Z"},"trusted":true},"outputs":[],"source":["print(df_train_set.head(),'\\n')\n","print(df_train_set.info(), '\\n')\n","\n","print(df_valid_set.head(),'\\n')\n","print(df_valid_set.info(), '\\n')\n","\n","print(df_test_set.head(),'\\n')\n","print(df_test_set.info(), '\\n')"]},{"cell_type":"markdown","metadata":{},"source":["**Let's create some barplots that illustrate the number of tweets of each sentiment for each party**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T14:49:25.922797Z","iopub.status.busy":"2023-11-21T14:49:25.922417Z","iopub.status.idle":"2023-11-21T14:49:25.949301Z","shell.execute_reply":"2023-11-21T14:49:25.947781Z","shell.execute_reply.started":"2023-11-21T14:49:25.922763Z"},"trusted":true},"outputs":[],"source":["group_df_by_sentiment_party_train = df_train_set.groupby(['Sentiment', 'Party']).size().reset_index(name='NumOfTweets')\n","group_df_by_sentiment_party_valid = df_valid_set.groupby(['Sentiment', 'Party']).size().reset_index(name='NumOfTweets')"]},{"cell_type":"markdown","metadata":{},"source":["**Below we can observe the number of tweets and their sentiment for each party for the train and valid sets**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T14:49:25.951727Z","iopub.status.busy":"2023-11-21T14:49:25.951354Z","iopub.status.idle":"2023-11-21T14:49:26.436990Z","shell.execute_reply":"2023-11-21T14:49:26.435402Z","shell.execute_reply.started":"2023-11-21T14:49:25.951698Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(12,8))\n","sns.barplot(x='Party', y='NumOfTweets', hue='Sentiment', data=group_df_by_sentiment_party_train)\n","plt.title('Number of Tweets/Sentiment per Party for Train set')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T14:49:26.439504Z","iopub.status.busy":"2023-11-21T14:49:26.439111Z","iopub.status.idle":"2023-11-21T14:49:26.910996Z","shell.execute_reply":"2023-11-21T14:49:26.909576Z","shell.execute_reply.started":"2023-11-21T14:49:26.439471Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(12,8))\n","sns.barplot(x='Party', y='NumOfTweets', hue='Sentiment', data=group_df_by_sentiment_party_valid)\n","plt.title('Number of Tweets/Sentiment per Party for Valid set')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["**Plot of the number of tweets for concerning each party of the test set**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T14:49:26.912843Z","iopub.status.busy":"2023-11-21T14:49:26.912509Z","iopub.status.idle":"2023-11-21T14:49:27.276302Z","shell.execute_reply":"2023-11-21T14:49:27.274735Z","shell.execute_reply.started":"2023-11-21T14:49:26.912813Z"},"trusted":true},"outputs":[],"source":["df_test_set['Party'].value_counts().plot(kind='bar', figsize=(12,8))\n","plt.title('Number of Tweets/Party')\n","plt.ylabel('Num of Tweets')\n","plt.xlabel('Party')\n","plt.xticks(rotation=45)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["\n","<h1>Data Preprocessing</h1>"]},{"cell_type":"markdown","metadata":{},"source":["**Turn the categorical classes of the train and the validation sets to numerical**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T14:49:27.278258Z","iopub.status.busy":"2023-11-21T14:49:27.277791Z","iopub.status.idle":"2023-11-21T14:49:27.290041Z","shell.execute_reply":"2023-11-21T14:49:27.288257Z","shell.execute_reply.started":"2023-11-21T14:49:27.278215Z"},"trusted":true},"outputs":[],"source":["df_train_set['Sentiment'].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T14:49:27.293081Z","iopub.status.busy":"2023-11-21T14:49:27.292656Z","iopub.status.idle":"2023-11-21T14:49:27.325350Z","shell.execute_reply":"2023-11-21T14:49:27.323100Z","shell.execute_reply.started":"2023-11-21T14:49:27.293048Z"},"trusted":true},"outputs":[],"source":["le = LabelEncoder()\n","\n","df_train_set['Sentiment'] = le.fit_transform(df_train_set['Sentiment'])\n","df_valid_set['Sentiment'] = le.fit_transform(df_valid_set['Sentiment'])\n","print(df_train_set['Sentiment'].head())\n","print(df_valid_set['Sentiment'].head())"]},{"cell_type":"markdown","metadata":{},"source":["**Now let's create a function that turn the text of each tweet to lowercase, removes stopwords and special charachters, urls, mentions e.t.c**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T14:49:27.330764Z","iopub.status.busy":"2023-11-21T14:49:27.330145Z","iopub.status.idle":"2023-11-21T14:49:27.345578Z","shell.execute_reply":"2023-11-21T14:49:27.344194Z","shell.execute_reply.started":"2023-11-21T14:49:27.330717Z"},"trusted":true},"outputs":[],"source":["# NOTE: To remove the stopwords I downloaded locally the stopwords-el.json file from the repository\n","# at https://github.com/stopwords-iso/stopwords-el and uploaded it\n","# to my notebook at gree-stopwords-json-file.\n","\n","# Load Greek stopwords from the JSON file\n","with open('/kaggle/input/greek-stopwords-2/stopwords_el_2.json', 'r', encoding='utf-8') as file:\n","    greek_stopwords = json.load(file)\n","\n","def preprocess_tweet(tweet):\n","    tweet = tweet.lower().replace('_', ' ')\n","    \n","    # delete mentions\n","    tweet = re.sub(r'@\\w+', '', tweet)\n","    \n","    # delete urls\n","    tweet = re.sub(r'http\\S+', '', tweet)\n","    \n","    # delete special characters but keep the alphanumeric ones, including all Greek letters\n","    #tweet = re.sub(r'[^αβγδεζηθικλμνξοπρστυφχψωςάέίόώύήΑΒΓΔΕΖΗΘΙΚΛΜΝΞΟΠΡΣΤΥΦΧΨΩa-zA-Z0-9\\s]', '', tweet)\n","    \n","    # I keep only greek charachters (I used to keep and english, I am trying it this way to see if I\n","    # will achieve higher f1 score)\n","    tweet = re.sub(r'[^αβγδεζηθικλμνξοπρστυφχψωςάέίόώύήΑΒΓΔΕΖΗΘΙΚΛΜΝΞΟΠΡΣΤΥΦΧΨΩ0-9\\s]', '', tweet)\n","    \n","    # delete Greek stopwords\n","    tweet_words = tweet.split()\n","    cleaned_words = [word for word in tweet_words if word not in greek_stopwords]\n","    tweet = ' '.join(cleaned_words)\n","\n","    tweet = tweet.strip()\n","    \n","    return tweet\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T14:49:27.349113Z","iopub.status.busy":"2023-11-21T14:49:27.348179Z","iopub.status.idle":"2023-11-21T14:49:40.607849Z","shell.execute_reply":"2023-11-21T14:49:40.606389Z","shell.execute_reply.started":"2023-11-21T14:49:27.349058Z"},"trusted":true},"outputs":[],"source":["df_train_set['Text'] = df_train_set['Text'].apply(preprocess_tweet)\n","df_test_set['Text'] = df_test_set['Text'].apply(preprocess_tweet)\n","df_valid_set['Text'] = df_valid_set['Text'].apply(preprocess_tweet)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T14:49:40.612928Z","iopub.status.busy":"2023-11-21T14:49:40.612565Z","iopub.status.idle":"2023-11-21T14:49:40.622357Z","shell.execute_reply":"2023-11-21T14:49:40.621213Z","shell.execute_reply.started":"2023-11-21T14:49:40.612897Z"},"trusted":true},"outputs":[],"source":["print(df_train_set['Text'].head(), '\\n')\n","print(df_test_set['Text'].head(), '\\n')\n","print(df_valid_set['Text'].head(), '\\n')"]},{"cell_type":"markdown","metadata":{},"source":["****"]},{"cell_type":"markdown","metadata":{},"source":["**We load the spacy model to perform lemmatization tokenaziation for greek words** <br>\n","Sometimes it's necessary to restart the kernel in order for the following to work"]},{"cell_type":"markdown","metadata":{},"source":["<h2>ATTENTION:</h3><h2>The following command needs to be executed only one time. If an error occurs from the following spacy.load() command just restart the kernel and run all the commands except this one.</h4>"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%capture\n","# This needs to be executed only one time. If an error occurs from the following spacy.load() command\n","# just restart the kernel and run all the commands except this one.\n","!pip install -U spacy  "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T14:49:40.626136Z","iopub.status.busy":"2023-11-21T14:49:40.624703Z","iopub.status.idle":"2023-11-21T14:49:48.755093Z","shell.execute_reply":"2023-11-21T14:49:48.753442Z","shell.execute_reply.started":"2023-11-21T14:49:40.626066Z"},"trusted":true},"outputs":[],"source":["nlp = spacy.load('/kaggle/input/el-core-news-lg-3/el_core_news_lg_3/el_core_news_lg-3.7.0')"]},{"cell_type":"markdown","metadata":{},"source":["**Below we perform lemmatization and tokenization**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T14:58:02.232093Z","iopub.status.busy":"2023-11-21T14:58:02.231528Z","iopub.status.idle":"2023-11-21T15:06:07.847808Z","shell.execute_reply":"2023-11-21T15:06:07.845961Z","shell.execute_reply.started":"2023-11-21T14:58:02.232040Z"},"trusted":true},"outputs":[],"source":["# For the lemmatization tokenazation step, I downloaded locally the el_core_news_lg model,\n","# then I zipped it and I uploaded it as a public dataset.\n","def lemmatize_tokenize_text(text):\n","    doc = nlp(text)\n","    return ' '.join([token.lemma_ for token in doc])\n","\n","\n","df_train_set['Text'] = df_train_set['Text'].apply(lemmatize_tokenize_text)\n","df_test_set['Text'] = df_test_set['Text'].apply(lemmatize_tokenize_text)\n","df_valid_set['Text'] = df_valid_set['Text'].apply(lemmatize_tokenize_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:06:07.850556Z","iopub.status.busy":"2023-11-21T15:06:07.850172Z","iopub.status.idle":"2023-11-21T15:06:07.861636Z","shell.execute_reply":"2023-11-21T15:06:07.859872Z","shell.execute_reply.started":"2023-11-21T15:06:07.850522Z"},"trusted":true},"outputs":[],"source":["print(df_train_set['Text'].head(), '\\n')\n","print(df_test_set['Text'].head(), '\\n')\n","print(df_valid_set['Text'].head(), '\\n')"]},{"cell_type":"markdown","metadata":{},"source":["Now we will calculate the number of unique words in each **Text** column and also plot a **Word cloud** for each of these columns"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:06:07.864451Z","iopub.status.busy":"2023-11-21T15:06:07.863836Z","iopub.status.idle":"2023-11-21T15:06:07.873169Z","shell.execute_reply":"2023-11-21T15:06:07.871955Z","shell.execute_reply.started":"2023-11-21T15:06:07.864385Z"},"trusted":true},"outputs":[],"source":["def unique_words_num(tweets):\n","    # Function that counts the number of the unique words from the Text column of each dataframe\n","    words = set() \n","    for tweet in tweets:\n","        words.update(tweet.split())\n","    return len(words)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:06:07.877038Z","iopub.status.busy":"2023-11-21T15:06:07.876490Z","iopub.status.idle":"2023-11-21T15:06:08.074623Z","shell.execute_reply":"2023-11-21T15:06:08.073054Z","shell.execute_reply.started":"2023-11-21T15:06:07.876995Z"},"trusted":true},"outputs":[],"source":["print(\"Num of unique words in df_train_set:\", unique_words_num(df_train_set['Text']))\n","print(\"Num of unique words in df_test_set:\", unique_words_num(df_test_set['Text']))\n","print(\"Num of unique words in df_valid_set:\", unique_words_num(df_valid_set['Text']))"]},{"cell_type":"markdown","metadata":{},"source":["Now let's plot the wordcloud for each dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:06:08.076234Z","iopub.status.busy":"2023-11-21T15:06:08.075908Z","iopub.status.idle":"2023-11-21T15:06:20.845760Z","shell.execute_reply":"2023-11-21T15:06:20.844205Z","shell.execute_reply.started":"2023-11-21T15:06:08.076205Z"},"trusted":true},"outputs":[],"source":["def plot_wordcloud(tweets, title):\n","    tweets_joined = ' '.join(tweets)\n","    wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=200).generate(tweets_joined)\n","    plt.figure(figsize=(10, 5))\n","    plt.imshow(wordcloud, interpolation='bilinear')\n","    plt.axis('off')\n","    plt.title(title)\n","    plt.show()\n","\n","# Plot word cloud for each dataframe\n","plot_wordcloud(df_train_set['Text'], 'df_train_set')\n","plot_wordcloud(df_test_set['Text'], 'df_test_set')\n","plot_wordcloud(df_valid_set['Text'], 'df_valid_set')"]},{"cell_type":"markdown","metadata":{},"source":["<h1>TF-IDF Vectorizer</h1>\n","Now we will use the TF-IDF Vectorizer to transform our text data to numerical data that can be interpreted by our model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:06:20.849038Z","iopub.status.busy":"2023-11-21T15:06:20.847836Z","iopub.status.idle":"2023-11-21T15:06:20.855253Z","shell.execute_reply":"2023-11-21T15:06:20.854192Z","shell.execute_reply.started":"2023-11-21T15:06:20.848986Z"},"trusted":true},"outputs":[],"source":["Y_train = df_train_set['Sentiment']\n","Y_val = df_valid_set['Sentiment']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:06:20.857608Z","iopub.status.busy":"2023-11-21T15:06:20.856772Z","iopub.status.idle":"2023-11-21T15:06:20.883030Z","shell.execute_reply":"2023-11-21T15:06:20.881153Z","shell.execute_reply.started":"2023-11-21T15:06:20.857558Z"},"trusted":true},"outputs":[],"source":["def choose_max_features(df_train_set, df_test_set, df_valid_set, Y_train, Y_val, max_features):\n","    tf_vec = TfidfVectorizer(max_features=max_features)\n","    X_train_tf = tf_vec.fit_transform(df_train_set['Text'])\n","    X_test_tf = tf_vec.transform(df_test_set['Text'])\n","    X_valid_tf = tf_vec.transform(df_valid_set['Text'])\n","    \n","    list_f1 = []\n","    list_f1_train = []\n","    list_sample_size = []\n","\n","    for times in range(9):\n","        X_subset, _, y_subset, _ = train_test_split(X_train_tf, Y_train, train_size=(times * 0.1 + 0.1))\n","        classifier = LogisticRegression(max_iter=2000)\n","        classifier.fit(X_subset, y_subset)\n","\n","        results_train = classifier.predict(X_subset)\n","        results = classifier.predict(X_valid_tf)\n","\n","        f1_train = f1_score(y_subset, results_train, average='macro')\n","        f1 = f1_score(Y_val, results, average='macro')\n","\n","        list_f1.append(f1)\n","        list_f1_train.append(f1_train)\n","        list_sample_size.append((times * 0.1 + 0.1))\n","\n","    mean_f1_train = np.mean(list_f1_train)\n","    mean_f1_validation = np.mean(list_f1)\n","\n","    return list_sample_size, list_f1, list_f1_train, mean_f1_train, mean_f1_validation, X_train_tf, X_test_tf, X_valid_tf\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:06:20.886275Z","iopub.status.busy":"2023-11-21T15:06:20.885863Z","iopub.status.idle":"2023-11-21T15:06:20.902357Z","shell.execute_reply":"2023-11-21T15:06:20.900803Z","shell.execute_reply.started":"2023-11-21T15:06:20.886237Z"},"trusted":true},"outputs":[],"source":["def plot_learning_curve(list_sample_size, list_f1, list_f1_train):\n","    plt.plot(list_sample_size, list_f1)\n","    plt.plot(list_sample_size, list_f1_train)\n","    plt.ylim(ymin=0)\n","    plt.legend([\"Validation\", \"Training\"])\n","    plt.xlabel(\"Proportion of Training Data\")\n","    plt.ylabel(\"F1 Score\")\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["**Below we experimenting with different values for the max_features hyperparameter**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:06:20.905178Z","iopub.status.busy":"2023-11-21T15:06:20.904094Z","iopub.status.idle":"2023-11-21T15:10:02.204396Z","shell.execute_reply":"2023-11-21T15:10:02.202721Z","shell.execute_reply.started":"2023-11-21T15:06:20.905133Z"},"trusted":true},"outputs":[],"source":["f1_scores_data = []\n","\n","for i in range(1000, 12000, 2000):\n","    print(f'\\n\\n FOR max_features = {i} \\n\\n')\n","    list_sample_size, list_f1, list_f1_train, mean_f1_train, mean_f1_validation, _, _, _ = choose_max_features(df_train_set, df_test_set, df_valid_set, Y_train, Y_val, i)\n","    f1_scores_data.append({'max_features': i, 'mean_f1_train': mean_f1_train, 'mean_f1_validation': mean_f1_validation})\n","    plot_learning_curve(list_sample_size, list_f1, list_f1_train)\n","\n","f1_scores_df = pd.DataFrame(f1_scores_data)\n","print(f1_scores_df)\n","\n","best_max_features = f1_scores_df.loc[f1_scores_df['mean_f1_validation'].idxmax(), 'max_features']\n","print(f'The best value for the max_features hyperparameter is {best_max_features}')"]},{"cell_type":"markdown","metadata":{},"source":["We will also experiment with the **ngram_range** hyperparameter"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:10:02.212276Z","iopub.status.busy":"2023-11-21T15:10:02.211794Z","iopub.status.idle":"2023-11-21T15:10:02.226851Z","shell.execute_reply":"2023-11-21T15:10:02.225165Z","shell.execute_reply.started":"2023-11-21T15:10:02.212239Z"},"trusted":true},"outputs":[],"source":["def choose_ngram_range(df_train_set, df_test_set, df_valid_set, Y_train, Y_val, ngram_range):\n","    tf_vec = TfidfVectorizer(ngram_range=ngram_range)\n","    X_train_tf = tf_vec.fit_transform(df_train_set['Text'])\n","    X_test_tf = tf_vec.transform(df_test_set['Text'])\n","    X_valid_tf = tf_vec.transform(df_valid_set['Text'])\n","    \n","    list_f1 = []\n","    list_f1_train = []\n","    list_sample_size = []\n","\n","    for times in range(9):\n","        X_subset, _, y_subset, _ = train_test_split(X_train_tf, Y_train, train_size=(times * 0.1 + 0.1))\n","        classifier = LogisticRegression(max_iter=2000)\n","        classifier.fit(X_subset, y_subset)\n","\n","        results_train = classifier.predict(X_subset)\n","        results = classifier.predict(X_valid_tf)\n","\n","        f1_train = f1_score(y_subset, results_train, average='macro')\n","        f1 = f1_score(Y_val, results, average='macro')\n","\n","        list_f1.append(f1)\n","        list_f1_train.append(f1_train)\n","        list_sample_size.append((times * 0.1 + 0.1))\n","\n","    mean_f1_train = np.mean(list_f1_train)\n","    mean_f1_validation = np.mean(list_f1)\n","\n","    return list_sample_size, list_f1, list_f1_train, mean_f1_train, mean_f1_validation, X_train_tf, X_test_tf, X_valid_tf\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:10:02.228972Z","iopub.status.busy":"2023-11-21T15:10:02.228379Z","iopub.status.idle":"2023-11-21T15:22:39.214974Z","shell.execute_reply":"2023-11-21T15:22:39.213639Z","shell.execute_reply.started":"2023-11-21T15:10:02.228925Z"},"trusted":true},"outputs":[],"source":["ngram_ranges = [\n","    (1, 1),  \n","    (1, 2),  \n","    (2, 2)]#,  \n","    #(1, 3),  \n","    #(2, 3),  \n","    #(1, 4)   \n","#] \n","ngram_scores_data = []\n","\n","for ngram in ngram_ranges:\n","    print(f'\\n\\n FOR ngram_range = {ngram} \\n\\n')\n","    list_sample_size, list_f1, list_f1_train, mean_f1_train, mean_f1_validation, _, _, _ = choose_ngram_range(df_train_set, df_test_set, df_valid_set, Y_train, Y_val, ngram)\n","    ngram_scores_data.append({'ngram_range': ngram, 'mean_f1_train': mean_f1_train, 'mean_f1_validation': mean_f1_validation})\n","    plot_learning_curve(list_sample_size, list_f1, list_f1_train)\n","\n","ngram_scores_df = pd.DataFrame(ngram_scores_data)\n","print(ngram_scores_df)\n","\n","best_ngram_range = ngram_scores_df.loc[ngram_scores_df['mean_f1_validation'].idxmax(), 'ngram_range']\n","print(f'The best ngram_range for the TF-IDF vectorizer is {best_ngram_range}')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:22:39.216938Z","iopub.status.busy":"2023-11-21T15:22:39.216574Z","iopub.status.idle":"2023-11-21T15:22:42.753539Z","shell.execute_reply":"2023-11-21T15:22:42.752353Z","shell.execute_reply.started":"2023-11-21T15:22:39.216905Z"},"trusted":true},"outputs":[],"source":["tf_vec = TfidfVectorizer(max_features=1000, ngram_range = (1,2)) # default ngram_range = (1,1)\n","X_train_tf = tf_vec.fit_transform(df_train_set['Text'])\n","X_test_tf = tf_vec.transform(df_test_set['Text'])\n","X_valid_tf = tf_vec.transform(df_valid_set['Text'])"]},{"cell_type":"markdown","metadata":{},"source":["<h1> Model hyperparameters experiments </h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:22:42.755527Z","iopub.status.busy":"2023-11-21T15:22:42.755122Z","iopub.status.idle":"2023-11-21T15:22:42.764973Z","shell.execute_reply":"2023-11-21T15:22:42.763151Z","shell.execute_reply.started":"2023-11-21T15:22:42.755495Z"},"trusted":true},"outputs":[],"source":["solvers = ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag']\n","penalty = ['l1', 'l2', 'elasticnet']\n","C = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n","random_state = [1, 2, 3, None]\n","max_iter = list(range(1000, 10000, 1000))\n","multi_class = ['auto', 'ovr', 'multinomial']\n","combinations = list(itertools.product(solvers, penalty, C, random_state, max_iter, multi_class))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:22:42.767202Z","iopub.status.busy":"2023-11-21T15:22:42.766852Z","iopub.status.idle":"2023-11-21T15:22:42.782543Z","shell.execute_reply":"2023-11-21T15:22:42.780760Z","shell.execute_reply.started":"2023-11-21T15:22:42.767174Z"},"trusted":true},"outputs":[],"source":["def plot_learning_curves(list_sample_size, list_f1, list_f1_train, list_recall, list_recall_train, list_precision, list_precision_train):\n","    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n","    \n","    #f1 plot\n","    axs[0].plot(list_sample_size, list_f1, 'b', label='Validation F1')\n","    axs[0].plot(list_sample_size, list_f1_train, 'r', label='Training F1')\n","    axs[0].set_title('F1 Score vs. Proportion of Training Data')\n","    axs[0].set_xlabel('Proportion of Training Data')\n","    axs[0].set_ylabel('F1 Score')\n","    axs[0].legend()\n","    axs[0].set_ylim([0, 1])\n","\n","    #recall plot\n","    axs[1].plot(list_sample_size, list_recall, 'b', label='Validation Recall')\n","    axs[1].plot(list_sample_size, list_recall_train, 'r', label='Training Recall')\n","    axs[1].set_title('Recall vs. Proportion of Training Data')\n","    axs[1].set_xlabel('Proportion of Training Data')\n","    axs[1].set_ylabel('Recall')\n","    axs[1].legend()\n","    axs[1].set_ylim([0, 1])\n","\n","    #precision plot\n","    axs[2].plot(list_sample_size, list_precision, 'b', label='Validation Precision')\n","    axs[2].plot(list_sample_size, list_precision_train, 'r', label='Training Precision')\n","    axs[2].set_title('Precision vs. Proportion of Training Data')\n","    axs[2].set_xlabel('Proportion of Training Data')\n","    axs[2].set_ylabel('Precision')\n","    axs[2].legend()\n","    axs[2].set_ylim([0, 1])\n","\n","    plt.tight_layout()\n","    plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["<h2>Experiments with iterations</h2>\n","\n","The number of iterations are the number of times the algorithm will loop through the entire training dataset. Each iteration is a step towards optimizing the weights of the features in the dataset, with the goal being that of minimizing the error of the prediction the model made. <br>\n","\n","**During each iteration, the algorithm performs the following steps:**\n","<ul>\n","    <li>Calculation: It calculates the predictions based on the current weights.</li>\n","    <li>Comparison: It compares the predictions with the actual targets.</li>\n","    <li>Error Measurement: It measures the error of these predictions.</li>\n","    <li>Adjustment: It adjusts the weights to reduce the error.</li>\n","</ul>\n","\n","These steps are repeated for the specified number of iterations or until the improvement becomes negligibly small, which indicates that the model has converged to a solution. <br>\n","\n","The number of iterations is a critical hyperparameter. Too few iterations might mean the model hasn't learned enough from the data, resulting in underfitting, while too many iterations can lead to overfitting, where the model learns too much from the training data, including the noise, leading to poor generalization on unseen data."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:22:42.785429Z","iopub.status.busy":"2023-11-21T15:22:42.784938Z","iopub.status.idle":"2023-11-21T15:22:42.806728Z","shell.execute_reply":"2023-11-21T15:22:42.804689Z","shell.execute_reply.started":"2023-11-21T15:22:42.785393Z"},"trusted":true},"outputs":[],"source":["def test_iterations(X_train_tf, X_valid_tf, Y_train, Y_val, max_iter_list):\n","    best_f1 = -1\n","    best_iter = None\n","    iter_results = []\n","    best_conf_matrix = None\n","\n","    for itr in max_iter_list:\n","        list_f1 = []\n","        list_f1_train = []\n","        list_sample_size = []\n","\n","        for times in range(1, 10):\n","            #split the data\n","            X_subset, _, y_subset, _ = train_test_split(X_train_tf, Y_train, train_size=times/10)\n","            classifier = LogisticRegression(max_iter=itr)\n","            classifier.fit(X_subset, y_subset)\n","\n","            #make predictions\n","            y_pred_train = classifier.predict(X_subset)\n","            y_pred_val = classifier.predict(X_valid_tf)\n","\n","            #calculate f1 train and f1 validation\n","            f1_train = f1_score(y_subset, y_pred_train, average='macro')\n","            f1_val = f1_score(Y_val, y_pred_val, average='macro')\n","\n","            list_f1_train.append(f1_train)\n","            list_f1.append(f1_val)\n","\n","        \n","        avg_f1 = np.mean(list_f1)\n","        if avg_f1 > best_f1:\n","            best_f1 = avg_f1\n","            best_iter = itr\n","            best_conf_matrix = confusion_matrix(Y_val, y_pred_val)\n","\n","        \n","        iter_results.append({\n","            'max_iter': itr,\n","            'mean_f1_train': np.mean(list_f1_train),\n","            'mean_f1_validation': np.mean(list_f1)\n","        })\n","\n","        # plot learning curve\n","        plt.figure()\n","        plt.plot(range(1, 10), list_f1_train, label='Training F1')\n","        plt.plot(range(1, 10), list_f1, label='Validation F1')\n","        plt.title(f'Learning Curves for max_iter={itr}')\n","        plt.xlabel('Proportion of Training Data')\n","        plt.ylabel('F1 Score')\n","        plt.legend()\n","        plt.show()\n","\n","    results_df = pd.DataFrame(iter_results)\n","    print(results_df)\n","\n","\n","    if best_conf_matrix is not None:\n","        sns.heatmap(best_conf_matrix, annot=True, fmt='d', cmap='Blues')\n","        plt.title(f'Confusion Matrix for max_iter={best_iter}')\n","        plt.xlabel('Predicted Label')\n","        plt.ylabel('True Label')\n","        plt.show()\n","\n","    return best_iter, results_df"]},{"cell_type":"markdown","metadata":{},"source":["Usually the values that the above function returns for **max_iter** hyperparameter is **1000**, **4000** and **9000**. I chose to use the 9000 for the max_iter so to avoid the ITERATIONS REACHED LIMIT error."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:22:42.810382Z","iopub.status.busy":"2023-11-21T15:22:42.809564Z","iopub.status.idle":"2023-11-21T15:24:01.729154Z","shell.execute_reply":"2023-11-21T15:24:01.727877Z","shell.execute_reply.started":"2023-11-21T15:22:42.810296Z"},"trusted":true},"outputs":[],"source":["best_max_iter, iter_df = test_iterations(X_train_tf, X_valid_tf, Y_train, Y_val, max_iter)\n","print(f\"The best max_iter is: {best_max_iter}\")"]},{"cell_type":"markdown","metadata":{},"source":["<h2> Experiments with solvers and penalty </h2>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:24:01.732087Z","iopub.status.busy":"2023-11-21T15:24:01.730991Z","iopub.status.idle":"2023-11-21T15:24:01.738398Z","shell.execute_reply":"2023-11-21T15:24:01.736808Z","shell.execute_reply.started":"2023-11-21T15:24:01.732032Z"},"trusted":true},"outputs":[],"source":["# since the number of iterations which gave me the best results is 4000 I will assign that number\n","# to a variable and I will proceed like that similarly in the following experiments.\n","best_iter = 4000 "]},{"cell_type":"markdown","metadata":{},"source":["The **solver** and **penalty** hyperparameters in Logistic Regression are crucial as they define the optimization algorithm used for minimizing the cost function and the regularization technique applied to prevent overfitting, respectively.<br>\n","\n","**Solver**: specifies the algorithm to use in the optimization problem.<br>\n","<ul>\n","<li>lbfgs, newton-cg, and sag are more suitable for large datasets as they handle multinomial loss and produce more robust results for multi-class problems.</li>\n","<li>liblinear is a good choice for small datasets and supports both l1 and l2 regularization.</li>\n","<li>saga is a variant of sag that also supports the l1 penalty, and is generally faster for large datasets.</li>\n","</ul><br>\n","\n","**Penalty**: specifies the norm used in the penalization. Regularization is applied to the cost function to avoid overfitting by discouraging complex models:\n","<ul>\n","<li>l1 can lead to sparse models with coefficients that can be exactly zero. It’s useful for feature selection as it tends to shrink coefficients for less important features to zero.</li>\n","<li>l2 tends to shrink coefficients evenly but typically does not set them to zero. This is often more appropriate for problems with many correlated features.</li>\n","</ul>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:24:01.741356Z","iopub.status.busy":"2023-11-21T15:24:01.740917Z","iopub.status.idle":"2023-11-21T15:24:01.767586Z","shell.execute_reply":"2023-11-21T15:24:01.766160Z","shell.execute_reply.started":"2023-11-21T15:24:01.741309Z"},"trusted":true},"outputs":[],"source":["def test_solvers_and_penalty(X_train_tf, X_valid_tf, Y_train, Y_val, solvers, best_iter):\n","\n","\n","    best_f1 = -1\n","    best_solver = None\n","    best_penalty = None\n","    best_model = None\n","    sc = StandardScaler(with_mean=False)\n","\n","    # DataFrame to store F1 scores for each solver and penalty\n","    df_scores = pd.DataFrame(columns=['Solver', 'Penalty', 'F1 Train Score', 'F1 Validation Score'])\n","\n","    for solve in solvers:\n","        list_f1 = []\n","        list_f1_train = []\n","        list_recall = []\n","        list_recall_train = []\n","        list_precision = []\n","        list_precision_train = []\n","        list_sample_size = []\n","        current_f1_list = []\n","        \n","        if solve in ['lbfgs', 'newton-cg', 'newton-cholesky', 'sag']:\n","            penalty = 'l2'\n","            if solve == 'sag': # we need to scale our data\n","                \n","                X_train_tf = sc.fit_transform(X_train_tf)\n","                X_valid_tf = sc.transform(X_valid_tf)\n","        if solve == 'liblinear':\n","            penalty = 'l1'\n","            \n","\n","        \n","        for times in range(9):\n","\n","            X_subset, _, y_subset, _ = train_test_split(X_train_tf, Y_train, train_size=(times * 0.1 + 0.1))\n","\n","            classifier = LogisticRegression(max_iter=best_iter, solver=solve, penalty=penalty)\n","            classifier.fit(X_subset, y_subset)\n","\n","            results_train = classifier.predict(X_subset)\n","\n","\n","            results = classifier.predict(X_valid_tf)\n","\n","\n","            f1_train = f1_score(y_subset, results_train, average='macro')\n","            recall_train = recall_score(y_subset, results_train, average='macro')\n","            precision_train = precision_score(y_subset, results_train, average='macro')\n","\n","            list_f1_train.append(f1_train)\n","            list_recall_train.append(recall_train)\n","            list_precision_train.append(precision_train)\n","\n","\n","            f1 = f1_score(Y_val, results, average='macro')\n","            recall = recall_score(Y_val, results, average='macro')\n","            precision = precision_score(Y_val, results, average='macro')\n","\n","            list_f1.append(f1)\n","            list_recall.append(recall)\n","            list_precision.append(precision)\n","            list_sample_size.append((times * 0.1 + 0.1))\n","            \n","            current_f1_list.append(f1)\n","        \n","        avg_f1_train = np.mean(list_f1_train)\n","        avg_f1_validation = np.mean(list_f1)\n","        \n","\n","        temp_df = pd.DataFrame({\n","            'Solver': [solve], \n","            'Penalty': [penalty], \n","            'F1 Train Score': [avg_f1_train], \n","            'F1 Validation Score': [avg_f1_validation]\n","        })\n","        df_scores = pd.concat([df_scores, temp_df], ignore_index=True)\n","        #df_scores = pd.DataFrame(columns=['Solver', 'Penalty', 'F1 Train Score', 'F1 Validation Score'])\n","        \n","\n","        avg_f1 = np.mean(current_f1_list)\n","        if avg_f1 > best_f1:\n","            best_f1 = avg_f1\n","            best_solver = solve\n","            best_penalty = penalty\n","        \n","        print(f'\\n\\n FOR solver = {solve} AND penalty = {penalty}\\n\\n')\n","        plot_learning_curves(list_sample_size, list_f1, list_f1_train, list_recall, list_recall_train, list_precision, list_precision_train)\n","    print(f'Best F1 score {best_f1:.2f} achieved with {best_solver} solver and penalty {best_penalty}.')\n","\n","\n","    print(\"\\nF1 Scores for each Solver and Penalty:\")\n","    print(df_scores)\n","\n","    best_model = LogisticRegression(max_iter=best_iter, solver=best_solver, penalty=best_penalty)\n","    best_model.fit(X_train_tf, Y_train)\n","    best_predictions = best_model.predict(X_valid_tf)\n","    cm = confusion_matrix(Y_val, best_predictions)\n","\n","\n","    plt.figure(figsize=(10, 7))\n","    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n","    plt.title('Confusion Matrix')\n","    plt.ylabel('Actual Label')\n","    plt.xlabel('Predicted Label')\n","    plt.show()\n","    \n","    return best_solver, best_penalty"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:24:01.769732Z","iopub.status.busy":"2023-11-21T15:24:01.769313Z","iopub.status.idle":"2023-11-21T15:25:38.266649Z","shell.execute_reply":"2023-11-21T15:25:38.265072Z","shell.execute_reply.started":"2023-11-21T15:24:01.769700Z"},"trusted":true},"outputs":[],"source":["best_solver, best_penalty = test_solvers_and_penalty(X_train_tf, X_valid_tf, Y_train, Y_val, solvers, best_iter)\n","print(best_solver, best_penalty)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:25:38.268966Z","iopub.status.busy":"2023-11-21T15:25:38.268611Z","iopub.status.idle":"2023-11-21T15:25:38.275836Z","shell.execute_reply":"2023-11-21T15:25:38.274344Z","shell.execute_reply.started":"2023-11-21T15:25:38.268935Z"},"trusted":true},"outputs":[],"source":["best_solver, best_penalty = 'newton-cg', 'l2'"]},{"cell_type":"markdown","metadata":{},"source":["<h2>Experiments with C</h2><br>\n","\n","The C hyperparameter plays a critical role in controlling the trade-off between achieving a low error on the training data and maintaining a model that generalizes well to unseen data. <br>\n","This hyperparameter is directly tied to the regularization strength with its value being the inverse of the regularization strength. <br>\n","\n","**Low values of C**:<br>\n","\n","Increase the regularization strength, which creates simpler models that may underfit the training data. This is because the optimization function will prioritize the simplicity (smaller coefficients, depending on the norm used in penalization) of the model over fitting the training data perfectly.\n","Useful when we believe the data is very noisy and we need to prevent the model from learning the noise.<br>\n","\n","**High values of C**:<br>\n","\n","Decrease the regularization strength, allowing the models to become more complex and fit the training data better. This can lead to overfitting if the model starts to learn the noise and detailed fluctuations within the training data.\n","Useful when the model suffers from high bias, i.e., it is too simple and does not capture the underlying trends well."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:25:38.278058Z","iopub.status.busy":"2023-11-21T15:25:38.277721Z","iopub.status.idle":"2023-11-21T15:25:38.301309Z","shell.execute_reply":"2023-11-21T15:25:38.299280Z","shell.execute_reply.started":"2023-11-21T15:25:38.278029Z"},"trusted":true},"outputs":[],"source":["def test_C(X_train_tf, X_valid_tf, Y_train, Y_val, C, best_iter, best_solver, best_penalty):\n","\n","    \n","    best_f1 = -1\n","    best_C = None\n","    best_model = None\n","    #sc = StandardScaler(with_mean=False)\n","\n","    \n","    df_scores = pd.DataFrame(columns=['C', 'F1 Train Score', 'F1 Validation Score'])\n","\n","    for c in C:\n","        list_f1 = []\n","        list_f1_train = []\n","        list_recall = []\n","        list_recall_train = []\n","        list_precision = []\n","        list_precision_train = []\n","        list_sample_size = []\n","        current_f1_list = []\n","        \n","            \n","        for times in range(9):\n","            \n","            X_subset, _, y_subset, _ = train_test_split(X_train_tf, Y_train, train_size=(times * 0.1 + 0.1))\n","\n","            classifier = LogisticRegression(max_iter=best_iter, solver=best_solver, penalty=best_penalty, C=c)\n","            classifier.fit(X_subset, y_subset)\n","\n","            results_train = classifier.predict(X_subset)\n","\n","            \n","            results = classifier.predict(X_valid_tf)\n","\n","            \n","            f1_train = f1_score(y_subset, results_train, average='macro')\n","            recall_train = recall_score(y_subset, results_train, average='macro')\n","            precision_train = precision_score(y_subset, results_train, average='macro')\n","\n","            list_f1_train.append(f1_train)\n","            list_recall_train.append(recall_train)\n","            list_precision_train.append(precision_train)\n","\n","            \n","            f1 = f1_score(Y_val, results, average='macro')\n","            recall = recall_score(Y_val, results, average='macro')\n","            precision = precision_score(Y_val, results, average='macro')\n","\n","            list_f1.append(f1)\n","            list_recall.append(recall)\n","            list_precision.append(precision)\n","            list_sample_size.append((times * 0.1 + 0.1))\n","            \n","            current_f1_list.append(f1)\n","        \n","        avg_f1_train = np.mean(list_f1_train)\n","        avg_f1_validation = np.mean(list_f1)\n","        \n","        \n","        temp_df = pd.DataFrame({\n","            'C': [c], \n","            'F1 Train Score': [avg_f1_train], \n","            'F1 Validation Score': [avg_f1_validation]\n","        })\n","        df_scores = pd.concat([df_scores, temp_df], ignore_index=True)\n","        \n","        \n","        avg_f1 = np.mean(current_f1_list)\n","        if avg_f1 > best_f1:\n","            best_f1 = avg_f1\n","            best_C = c\n","\n","        \n","        print(f'\\n\\n FOR C = {c}\\n\\n')\n","        plot_learning_curves(list_sample_size, list_f1, list_f1_train, list_recall, list_recall_train, list_precision, list_precision_train)\n","    print(f'Best F1 score {best_f1:.2f} achieved with C = {best_C}.')\n","\n","    \n","    print(\"\\nF1 Scores for each value of C:\")\n","    print(df_scores)\n","\n","    best_model = LogisticRegression(max_iter=best_iter, solver=best_solver, penalty=best_penalty, C=best_C)\n","    best_model.fit(X_train_tf, Y_train)\n","    best_predictions = best_model.predict(X_valid_tf)\n","    cm = confusion_matrix(Y_val, best_predictions)\n","\n","    # plot the confusion matrix with seaborn\n","    plt.figure(figsize=(10, 7))\n","    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n","    plt.title('Confusion Matrix')\n","    plt.ylabel('Actual Label')\n","    plt.xlabel('Predicted Label')\n","    plt.show()\n","    \n","    return best_C"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:25:38.303382Z","iopub.status.busy":"2023-11-21T15:25:38.302972Z","iopub.status.idle":"2023-11-21T15:26:36.587959Z","shell.execute_reply":"2023-11-21T15:26:36.586044Z","shell.execute_reply.started":"2023-11-21T15:25:38.303314Z"},"trusted":true},"outputs":[],"source":["best_c = test_C(X_train_tf, X_valid_tf, Y_train, Y_val, C, best_iter, best_solver, best_penalty)\n","print(best_c)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:26:36.590169Z","iopub.status.busy":"2023-11-21T15:26:36.589736Z","iopub.status.idle":"2023-11-21T15:26:36.594397Z","shell.execute_reply":"2023-11-21T15:26:36.593454Z","shell.execute_reply.started":"2023-11-21T15:26:36.590135Z"},"trusted":true},"outputs":[],"source":["best_C = 0.1"]},{"cell_type":"markdown","metadata":{},"source":["<h2>Experiments with the multi_class hyperparameter</h2>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:26:36.595771Z","iopub.status.busy":"2023-11-21T15:26:36.595412Z","iopub.status.idle":"2023-11-21T15:26:36.611640Z","shell.execute_reply":"2023-11-21T15:26:36.610431Z","shell.execute_reply.started":"2023-11-21T15:26:36.595728Z"},"trusted":true},"outputs":[],"source":["multi_class_ls = ['auto', 'ovr', 'multinomial']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:26:36.614009Z","iopub.status.busy":"2023-11-21T15:26:36.613665Z","iopub.status.idle":"2023-11-21T15:26:36.638303Z","shell.execute_reply":"2023-11-21T15:26:36.636219Z","shell.execute_reply.started":"2023-11-21T15:26:36.613980Z"},"trusted":true},"outputs":[],"source":["def test_multi_class(X_train_tf, X_valid_tf, Y_train, Y_val, multi_class_ls, best_iter, best_solver, best_penalty , best_C):\n","\n","    \n","    best_f1 = -1  \n","    best_multi_class_val = None  \n","    best_model = None\n","\n","\n","\n","    df_scores = pd.DataFrame(columns=['multi_class', 'F1 Train Score', 'F1 Validation Score'])\n","\n","    for multi_class_val in multi_class_ls:\n","        list_f1 = []\n","        list_f1_train = []\n","        list_recall = []\n","        list_recall_train = []\n","        list_precision = []\n","        list_precision_train = []\n","        list_sample_size = []\n","        current_f1_list = []\n","        \n","            \n","        for times in range(9):\n","\n","            X_subset, _, y_subset, _ = train_test_split(X_train_tf, Y_train, train_size=(times * 0.1 + 0.1))\n","\n","            classifier = LogisticRegression(max_iter=best_iter, solver=best_solver, penalty=best_penalty, C=best_C, multi_class=multi_class_val)\n","            classifier.fit(X_subset, y_subset)\n","\n","            results_train = classifier.predict(X_subset)\n","\n","\n","            results = classifier.predict(X_valid_tf)\n","\n","\n","            f1_train = f1_score(y_subset, results_train, average='macro')\n","            recall_train = recall_score(y_subset, results_train, average='macro')\n","            precision_train = precision_score(y_subset, results_train, average='macro')\n","\n","            list_f1_train.append(f1_train)\n","            list_recall_train.append(recall_train)\n","            list_precision_train.append(precision_train)\n","\n","\n","            f1 = f1_score(Y_val, results, average='macro')\n","            recall = recall_score(Y_val, results, average='macro')\n","            precision = precision_score(Y_val, results, average='macro')\n","\n","            list_f1.append(f1)\n","            list_recall.append(recall)\n","            list_precision.append(precision)\n","            list_sample_size.append((times * 0.1 + 0.1))\n","            \n","            current_f1_list.append(f1)\n","        \n","        avg_f1_train = np.mean(list_f1_train)\n","        avg_f1_validation = np.mean(list_f1)\n","        \n","\n","        temp_df = pd.DataFrame({\n","            'multi_class': [multi_class_val], \n","            'F1 Train Score': [avg_f1_train], \n","            'F1 Validation Score': [avg_f1_validation]\n","        })\n","        df_scores = pd.concat([df_scores, temp_df], ignore_index=True)\n","        \n","\n","        avg_f1 = np.mean(current_f1_list)\n","        if avg_f1 > best_f1:\n","            best_f1 = avg_f1\n","            best_multi_class_val = multi_class_val\n","\n","        \n","        print(f'\\n\\n FOR multi_class = {multi_class_val}\\n\\n')\n","        plot_learning_curves(list_sample_size, list_f1, list_f1_train, list_recall, list_recall_train, list_precision, list_precision_train)\n","    print(f'Best F1 score {best_f1:.2f} achieved with multi_class = {best_multi_class_val}.')\n","\n","\n","    print(\"\\nF1 Scores for each value of multi_class:\")\n","    print(df_scores)\n","\n","    best_model = LogisticRegression(max_iter=best_iter, solver=best_solver, penalty=best_penalty, C=best_C, multi_class=best_multi_class_val)\n","    best_model.fit(X_train_tf, Y_train)\n","    best_predictions = best_model.predict(X_valid_tf)\n","    cm = confusion_matrix(Y_val, best_predictions)\n","\n","\n","    plt.figure(figsize=(10, 7))\n","    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n","    plt.title('Confusion Matrix')\n","    plt.ylabel('Actual Label')\n","    plt.xlabel('Predicted Label')\n","    plt.show()\n","    \n","    return best_multi_class_val"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:26:36.640599Z","iopub.status.busy":"2023-11-21T15:26:36.640104Z","iopub.status.idle":"2023-11-21T15:26:53.252090Z","shell.execute_reply":"2023-11-21T15:26:53.250724Z","shell.execute_reply.started":"2023-11-21T15:26:36.640556Z"},"trusted":true},"outputs":[],"source":["best_multiclass_val = test_multi_class(X_train_tf, X_valid_tf, Y_train, Y_val, multi_class_ls, best_iter, best_solver, best_penalty , best_C)\n","print(best_multiclass_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:26:53.255627Z","iopub.status.busy":"2023-11-21T15:26:53.254179Z","iopub.status.idle":"2023-11-21T15:26:53.262252Z","shell.execute_reply":"2023-11-21T15:26:53.260007Z","shell.execute_reply.started":"2023-11-21T15:26:53.255565Z"},"trusted":true},"outputs":[],"source":["best_multiclass_val = 'multinomial'"]},{"cell_type":"markdown","metadata":{},"source":["<h2> No I will combine the train set and the validation set and perform kfold crossvalidation</h2>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:26:53.271400Z","iopub.status.busy":"2023-11-21T15:26:53.270904Z","iopub.status.idle":"2023-11-21T15:26:53.283222Z","shell.execute_reply":"2023-11-21T15:26:53.281162Z","shell.execute_reply.started":"2023-11-21T15:26:53.271361Z"},"trusted":true},"outputs":[],"source":["def plot_validation_scores(average_f1, average_recall, average_precision):\n","    metrics = ['F1 Score', 'Recall', 'Precision']\n","    average_scores = [average_f1, average_recall, average_precision]\n","\n","    fig, ax = plt.subplots(figsize=(8, 6))\n","    ax.bar(metrics, average_scores, color=['blue', 'green', 'red'])\n","\n","    ax.set_title('Validation Scores')\n","    ax.set_ylim([0, 1])\n","    ax.set_ylabel('Scores')\n","    \n","    for i, v in enumerate(average_scores):\n","        ax.text(i, v + 0.02, f\"{v:.2f}\", ha='center', va='bottom')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:26:53.285413Z","iopub.status.busy":"2023-11-21T15:26:53.284808Z","iopub.status.idle":"2023-11-21T15:26:53.962033Z","shell.execute_reply":"2023-11-21T15:26:53.960938Z","shell.execute_reply.started":"2023-11-21T15:26:53.285318Z"},"trusted":true},"outputs":[],"source":["# Combine train and valid set\n","X_combined = np.vstack((X_train_tf.toarray(), X_valid_tf.toarray()))\n","y_combined = np.hstack((Y_train, Y_val))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:26:53.964864Z","iopub.status.busy":"2023-11-21T15:26:53.963620Z","iopub.status.idle":"2023-11-21T15:27:47.307731Z","shell.execute_reply":"2023-11-21T15:27:47.305750Z","shell.execute_reply.started":"2023-11-21T15:26:53.964827Z"},"trusted":true},"outputs":[],"source":["\n","kfold = StratifiedKFold(n_splits=5)\n","\n","\n","list_f1_scores_train = []\n","list_recall_scores_train = []\n","list_precision_scores_train = []\n","list_f1_scores_val = []\n","list_recall_scores_val = []\n","list_precision_scores_val = []\n","\n","\n","# k-fold cross-validation\n","for train_index, val_index in kfold.split(X_combined, y_combined):\n","\n","    X_train_fold, X_val_fold = X_combined[train_index], X_combined[val_index]\n","    y_train_fold, y_val_fold = y_combined[train_index], y_combined[val_index]\n","    \n","\n","    model = LogisticRegression(multi_class=best_multiclass_val, C=best_C, solver=best_solver, penalty=best_penalty, max_iter=best_iter)\n","    model.fit(X_train_fold, y_train_fold)\n","    \n","    predictions_train = model.predict(X_train_fold)\n","    f1_train = f1_score(y_train_fold, predictions_train, average='macro')\n","    recall_train = recall_score(y_train_fold, predictions_train, average='macro')\n","    precision_train = precision_score(y_train_fold, predictions_train, average='macro', zero_division=0)\n","    \n","    list_f1_scores_train.append(f1_train)\n","    list_recall_scores_train.append(recall_train)\n","    list_precision_scores_train.append(precision_train)\n","    \n","    predictions_val = model.predict(X_val_fold)\n","    f1_val = f1_score(y_val_fold, predictions_val, average='macro')\n","    recall_val = recall_score(y_val_fold, predictions_val, average='macro')\n","    precision_val = precision_score(y_val_fold, predictions_val, average='macro', zero_division=0)\n","    \n","\n","    list_f1_scores_val.append(f1_val)\n","    list_recall_scores_val.append(recall_val)\n","    list_precision_scores_val.append(precision_val)\n","\n","\n","    \n","average_f1_train = np.mean(list_f1_scores_train)\n","average_recall_train = np.mean(list_recall_scores_train)\n","average_precision_train = np.mean(list_precision_scores_train)    \n","\n","average_f1_val = np.mean(list_f1_scores_val)\n","average_recall_val = np.mean(list_recall_scores_val)\n","average_precision_val = np.mean(list_precision_scores_val)\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:27:47.312489Z","iopub.status.busy":"2023-11-21T15:27:47.311137Z","iopub.status.idle":"2023-11-21T15:27:47.665985Z","shell.execute_reply":"2023-11-21T15:27:47.664668Z","shell.execute_reply.started":"2023-11-21T15:27:47.312405Z"},"trusted":true},"outputs":[],"source":["print(f'F1 train: {average_f1_train} F1 val: {average_f1_val}')\n","plot_validation_scores(average_f1_val, average_recall_val, average_precision_val)"]},{"cell_type":"markdown","metadata":{},"source":["<h2>Experiments with the CountVectorizer</h2>\n","<p>Since we don't acieve high f1 score with the previous experiments we will try to use the CountVectorizer to see what scores we will achieve</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:27:47.667942Z","iopub.status.busy":"2023-11-21T15:27:47.667556Z","iopub.status.idle":"2023-11-21T15:27:49.276891Z","shell.execute_reply":"2023-11-21T15:27:49.275405Z","shell.execute_reply.started":"2023-11-21T15:27:47.667908Z"},"trusted":true},"outputs":[],"source":["count_vec = CountVectorizer(max_features=4000)\n","X_train_counts = count_vec.fit_transform(df_train_set['Text'])\n","X_test_counts = count_vec.transform(df_test_set['Text'])\n","X_valid_counts = count_vec.transform(df_valid_set['Text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:27:49.279272Z","iopub.status.busy":"2023-11-21T15:27:49.278843Z","iopub.status.idle":"2023-11-21T15:27:50.920396Z","shell.execute_reply":"2023-11-21T15:27:50.918740Z","shell.execute_reply.started":"2023-11-21T15:27:49.279236Z"},"trusted":true},"outputs":[],"source":["\n","X_combined = np.vstack((X_train_counts.toarray(), X_valid_counts.toarray()))\n","y_combined = np.hstack((Y_train, Y_val))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:27:50.921853Z","iopub.status.busy":"2023-11-21T15:27:50.921531Z","iopub.status.idle":"2023-11-21T15:27:50.928933Z","shell.execute_reply":"2023-11-21T15:27:50.927146Z","shell.execute_reply.started":"2023-11-21T15:27:50.921822Z"},"trusted":true},"outputs":[],"source":["best_model = LogisticRegression(multi_class=best_multiclass_val, C=best_C, solver=best_solver, penalty=best_penalty, max_iter=best_iter)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:27:50.931660Z","iopub.status.busy":"2023-11-21T15:27:50.931088Z","iopub.status.idle":"2023-11-21T15:27:51.586365Z","shell.execute_reply":"2023-11-21T15:27:51.584785Z","shell.execute_reply.started":"2023-11-21T15:27:50.931610Z"},"trusted":true},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X_combined, y_combined, test_size=0.33, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:27:51.588737Z","iopub.status.busy":"2023-11-21T15:27:51.588165Z","iopub.status.idle":"2023-11-21T15:29:15.636263Z","shell.execute_reply":"2023-11-21T15:29:15.632985Z","shell.execute_reply.started":"2023-11-21T15:27:51.588687Z"},"trusted":true},"outputs":[],"source":["model.fit(X_train, y_train)\n","predictions = model.predict(X_test)\n","\n","\n","f1 = f1_score(y_test, predictions, average='macro')\n","recall = recall_score(y_test, predictions, average='macro')\n","precision = precision_score(y_test, predictions, average='macro', zero_division=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:29:15.640633Z","iopub.status.busy":"2023-11-21T15:29:15.639262Z","iopub.status.idle":"2023-11-21T15:29:15.652387Z","shell.execute_reply":"2023-11-21T15:29:15.649615Z","shell.execute_reply.started":"2023-11-21T15:29:15.640571Z"},"trusted":true},"outputs":[],"source":["print(f'f1: {f1} Recall: {recall} Precision: {precision}')"]},{"cell_type":"markdown","metadata":{},"source":["<h1>Make predictions</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:29:15.656329Z","iopub.status.busy":"2023-11-21T15:29:15.655298Z","iopub.status.idle":"2023-11-21T15:29:16.286318Z","shell.execute_reply":"2023-11-21T15:29:16.285006Z","shell.execute_reply.started":"2023-11-21T15:29:15.656273Z"},"trusted":true},"outputs":[],"source":["X_combined = np.vstack((X_train_tf.toarray(), X_valid_tf.toarray()))\n","y_combined = np.hstack((Y_train, Y_val))\n","best_model = LogisticRegression(multi_class=best_multiclass_val, C=best_C, solver=best_solver, penalty=best_penalty, max_iter=best_iter)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:29:16.288491Z","iopub.status.busy":"2023-11-21T15:29:16.288082Z","iopub.status.idle":"2023-11-21T15:29:28.936105Z","shell.execute_reply":"2023-11-21T15:29:28.934545Z","shell.execute_reply.started":"2023-11-21T15:29:16.288455Z"},"trusted":true},"outputs":[],"source":["best_model.fit(X_combined, y_combined)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:29:28.940660Z","iopub.status.busy":"2023-11-21T15:29:28.939267Z","iopub.status.idle":"2023-11-21T15:29:28.967665Z","shell.execute_reply":"2023-11-21T15:29:28.965628Z","shell.execute_reply.started":"2023-11-21T15:29:28.940589Z"},"trusted":true},"outputs":[],"source":["predictions = best_model.predict(X_test_tf)\n","predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T15:29:28.972497Z","iopub.status.busy":"2023-11-21T15:29:28.970993Z","iopub.status.idle":"2023-11-21T15:29:29.060569Z","shell.execute_reply":"2023-11-21T15:29:29.059462Z","shell.execute_reply.started":"2023-11-21T15:29:28.972426Z"},"trusted":true},"outputs":[],"source":["categorical_predictions = le.inverse_transform(predictions)\n","\n","results = pd.DataFrame({\n","    'Id': df_test_set['New_ID'],\n","    'Predicted': categorical_predictions\n","})\n","\n","results.to_csv('submission.csv', index=False)\n","results"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":6886092,"sourceId":62857,"sourceType":"competition"},{"datasetId":3931388,"sourceId":6838093,"sourceType":"datasetVersion"},{"datasetId":3931665,"sourceId":6838701,"sourceType":"datasetVersion"},{"datasetId":4016768,"sourceId":6988816,"sourceType":"datasetVersion"},{"datasetId":4016909,"sourceId":6989017,"sourceType":"datasetVersion"}],"dockerImageVersionId":30558,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
