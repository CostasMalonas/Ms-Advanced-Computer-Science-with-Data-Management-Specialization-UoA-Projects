{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\n",
    "from joblib import dump, load\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "import hashlib\n",
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"imdb_train.csv\")\n",
    "df_test = pd.read_csv(\"imdb_test_without_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(reviews):\n",
    "    \n",
    "    \"\"\"Initialize an empty list to hold the clean reviews\"\"\"\n",
    "    clean_train_reviews = []\n",
    "\n",
    "    # Loop over each review in the list\n",
    "    for index, review in enumerate(reviews):\n",
    "        # Call the pre processer for each review, and add the result to the list of clean reviews\n",
    "        clean_train_reviews.append(preProcess(review))\n",
    "    \n",
    "    return clean_train_reviews\n",
    " \n",
    "def preProcess(rawReview):\n",
    "\n",
    "    \"\"\"Function to convert a raw review to a string of words\n",
    "        Takes in a raw movie review as a single string to output a preprocessed movie review as a single string\"\"\"\n",
    "\n",
    "    text_only = re.sub('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});', '', rawReview)\n",
    "    #\n",
    "    # 2. Remove Email IDs, URLs and numbers\n",
    "    noEmail = re.sub(r'([\\w\\.-]+@[\\w\\.-]+\\.\\w+)','',text_only)\n",
    "    \n",
    "    noUrl = re.sub(r'(?i)\\b((?:[a-z][\\w-]+:(?:/{1,3}|[a-z0-9%])|www\\d{0,3}[.]| \\\n",
    "        [a-z0-9.\\-]+[.][a-z]{2,4}/|[a-z0-9.\\-]+[.][a-z])(?:[^\\s()<>]+|\\(([^\\s()<>]+| \\\n",
    "        (\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))','', noEmail)\n",
    "    \n",
    "    #Emotional symbols may affect the meaning of the review\n",
    "    smileys = \"\"\":-) :) :o) :D :-D :( :-( :o(\"\"\".split()\n",
    "    smileyPattern = \"|\".join(map(re.escape, smileys))\n",
    "    \n",
    "    letters_only = re.sub(\"[^a-zA-Z\" + smileyPattern + \"]\", \" \", noUrl)\n",
    "    #\n",
    "    # 3. Convert to lower case and split into individual words\n",
    "    words = letters_only.lower().split()     \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words and also 3-letter words and Lemmatize the review\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = ''\n",
    "    for word in words:\n",
    "        if word not in stops and len(word) > 3:\n",
    "        #if len(word) > 3:\n",
    "            lemmatized_words += str(lemmatizer.lemmatize(word)) + ' '\n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space and return the result.\n",
    "    return lemmatized_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_review = clean(df_train['review'])\n",
    "clean_test_review = clean(df_test['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern=r'[^a-zA-z0-9\\s]'\n",
    "    text=re.sub(pattern,'',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_clean = df_train \n",
    "df_train_clean['review'] = clean_train_review\n",
    "df_train_clean['review'] = df_train_clean['review'].apply(remove_special_characters)\n",
    "df_test_clean = df_test\n",
    "df_test_clean['review'] = clean_test_review\n",
    "df_test_clean['review'] = df_test_clean['review'].apply(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_vectorizer = CountVectorizer(max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features = cnt_vectorizer.fit_transform(df_train_clean['review'][:10000]) #40000, 5000\n",
    "X_test_features = cnt_vectorizer.transform(df_test_clean['review'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Brute Force KNN</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features = X_train_features.toarray()\n",
    "X_test_features = X_test_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn =  NearestNeighbors(n_neighbors=15,algorithm='brute', metric=\"jaccard\")\n",
    "knn_brute = knn.fit(X_train_features, df_train_clean['sentiment'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "distances, indices = knn.kneighbors(X_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18368\n"
     ]
    }
   ],
   "source": [
    "print(df_train.iloc[indices[5][14]]['id']) #2913"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in the results_dict variable we will store to each test id (key of dictionary) a list containing id's of the neighbors that predicted from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "\n",
    "for i, idn in enumerate(df_test['id']):\n",
    "  results_dict[idn] = [df_train.iloc[indices[i][j]]['id'] for j in range(15)]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>LSH-MinHash</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsh = MinHashLSH(threshold=0.5, num_perm=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create hashes for the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building time between 45-50 minutes\n",
    "train_hashes = {}\n",
    "i = 0\n",
    "for id, row in zip(df_train_clean['id'][:10000], X_train_features):\n",
    "    m_temp = MinHash(num_perm=50)\n",
    "    for val in row:\n",
    "        m_temp.update(val)\n",
    "    train_hashes[id] = [m_temp, df_train_clean.iloc[i]['review'], df_train_clean.iloc[i]['sentiment']]\n",
    "    i += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in train_hashes.keys():\n",
    "    lsh.insert(key, train_hashes[key][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hashes = {}\n",
    "i = 0\n",
    "for id, row in zip(df_test_clean['id'], X_test_features):\n",
    "    m_temp = MinHash(num_perm=50)\n",
    "    for val in row:\n",
    "        m_temp.update(val)\n",
    "    test_hashes[id] = [m_temp, df_test_clean.iloc[i]['review']]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = [lsh.query(test_hashes[hash][0]) for hash in test_hashes.keys()] # 151 non empty lists"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Now we will find the closest neighbors to each test example</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\"\"\"\n",
    "Take each nested list from the test_query that contains the neighbor hashes(id's).\n",
    "Take the reviews associated with each id that is neighbor to that row.\n",
    "Transform the reviews to tfidf\n",
    "\"\"\"\n",
    "idx = 0\n",
    "results = {} # key is the test id and values are the id's (hashes) predicted\n",
    "for row in test_query:\n",
    "    if len(row) == 0:\n",
    "        results[df_test_clean.iloc[idx]['id']] = 0 \n",
    "        idx += 1\n",
    "        continue\n",
    "    if len(row) >= 15:\n",
    "        knn_temp = NearestNeighbors(n_neighbors=15,algorithm='brute', metric='jaccard')\n",
    "        df_temp = df_train_clean.loc[df_train_clean['id'].isin(row)] # get rows with specific id's that lsh query returned for the specific example\n",
    "        X_train_features_temp = cnt_vectorizer.transform(df_temp['review'])\n",
    "        X_train_features_temp = X_train_features_temp.toarray()\n",
    "\n",
    "        if X_train_features_temp.shape[1] < 5000:\n",
    "           results[df_test_clean.iloc[idx]['id']] = 0\n",
    "           idx += 1\n",
    "           continue\n",
    "\n",
    "        knn_temp.fit(X_train_features_temp, df_temp['sentiment'].to_numpy())\n",
    "        distances, indices = knn_temp.kneighbors([X_test_features[idx]])\n",
    "        if len(indices[0]) >= 15:\n",
    "            results[df_test_clean.iloc[idx]['id']] = [df_train.iloc[indices[0][j]]['id'] for j in range(15)]\n",
    "        else:\n",
    "            results[df_test_clean.iloc[idx]['id']] = [df_train.iloc[indices[0][j]]['id'] for j in range(len(indices))]\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key 33527 result [37355, 9022, 43936, 6752, 20693, 45687, 31907, 43190, 9333, 17819, 4185, 17399, 11796, 21506, 40878]\n"
     ]
    }
   ],
   "source": [
    "for key in results.keys():\n",
    "    if results[key] != 0:\n",
    "        print(f'key {key} result {results[key]}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_dict_ls = []\n",
    "for key in results_dict.keys():\n",
    "    if type(results_dict[key]) == int:\n",
    "        continue\n",
    "    for val in results_dict[key]:\n",
    "        results_dict_ls.append(val)\n",
    "\n",
    "results_ls = []\n",
    "for key in results.keys():\n",
    "    if type(results[key]) == int:\n",
    "        continue\n",
    "    for val in results[key]:\n",
    "        results_ls.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage similarity among lists is : 19.07001441250095\n"
     ]
    }
   ],
   "source": [
    "res = len(set(results_ls) & set(results_dict_ls)) / float(len(set(results_ls) | set(results_dict_ls))) * 100\n",
    "  \n",
    "# printing result\n",
    "print(\"Percentage similarity among lists is : \" + str(res))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Random projection</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbits = 4 # buckets \n",
    "d = 5000 # dimension \n",
    "plane_norms = np.random.rand(nbits, d) - .5 # hyperplaens centered around 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix_dot_products = np.dot(X_train_features, plane_norms.T)\n",
    "test_matrix_dot_products = np.dot(X_test_features, plane_norms.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix_dot_products = [row > 0 for row in train_matrix_dot_products]\n",
    "test_matrix_dot_products = [row > 0 for row in test_matrix_dot_products]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38405"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_false = [row for row in train_matrix_dot_products if True in row]\n",
    "len(not_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix_dot_products = [row.astype(int) for row in train_matrix_dot_products]\n",
    "test_matrix_dot_products = [row.astype(int) for row in test_matrix_dot_products]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets_train = {}\n",
    "for i in range(len(train_matrix_dot_products)):\n",
    "    hash_str = ''.join(train_matrix_dot_products[i].astype(str))\n",
    "    if hash_str not in buckets_train.keys():\n",
    "        buckets_train[hash_str] = []\n",
    "    # Μπαίνει το index των σειρών στην λίστα,    \n",
    "    #οπότε οι σειρές αυτές που έχουν το ίδιο κλειδί hash είναι γείτονεσ \n",
    "    buckets_train[hash_str].append(i)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets_test = {}\n",
    "for i in range(len(test_matrix_dot_products)):\n",
    "    hash_str = ''.join(test_matrix_dot_products[i].astype(str))\n",
    "    if hash_str not in buckets_test.keys():\n",
    "        buckets_test[hash_str] = []\n",
    "    # Μπαίνει το index των σειρών στην λίστα,    \n",
    "    #οπότε οι σειρές αυτές που έχουν το ίδιο κλειδί hash είναι γείτονεσ \n",
    "    buckets_test[hash_str].append(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_random = {}\n",
    "for key in buckets_test.keys():\n",
    "    try:\n",
    "        # if the key also exists in train buckets get the index\n",
    "        # of the rows that ended up in the same bucket\n",
    "        indexes = buckets_train[key] \n",
    "    except:\n",
    "        continue \n",
    "\n",
    "    df_temp = df_train_clean.iloc[indexes] # get the rows with the indexes list\n",
    "    X_train_features_temp = cnt_vectorizer.transform(df_temp['review'])\n",
    "    X_train_features_temp = X_train_features_temp.toarray()\n",
    "\n",
    "    if X_train_features_temp.shape[1] < 5000:\n",
    "        continue\n",
    "    for idx in buckets_test[key]:\n",
    "        knn_temp = NearestNeighbors(n_neighbors=len(indexes),algorithm='brute', metric='cosine')\n",
    "        knn_temp.fit(X_train_features_temp, df_temp['sentiment'].to_numpy())\n",
    "        distances, indices = knn_temp.kneighbors([X_test_features[idx]])\n",
    "        if len(indices[0]) >= 15:\n",
    "            results_random[df_test_clean.iloc[idx]['id']] = [df_train.iloc[indices[0][j]]['id'] for j in range(15)]\n",
    "        else:\n",
    "            results_random[df_test_clean.iloc[idx]['id']] = [df_train.iloc[indices[0][j]]['id'] for j in range(len(indices))]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_dict_ls = []\n",
    "for key in results_dict.keys():\n",
    "    if type(results_dict[key]) == int:\n",
    "        continue\n",
    "    for val in results_dict[key]:\n",
    "        results_dict_ls.append(val)\n",
    "\n",
    "results_ls = []\n",
    "for key in results_random.keys():\n",
    "    if type(results_random[key]) == int:\n",
    "        continue\n",
    "    for val in results_random[key]:\n",
    "        results_ls.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage similarity among lists is : 15.241081928655428\n"
     ]
    }
   ],
   "source": [
    "res = len(set(results_ls) & set(results_dict_ls)) / float(len(set(results_ls) | set(results_dict_ls))) * 100\n",
    "  \n",
    "# printing result\n",
    "print(\"Percentage similarity among lists is : \" + str(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15e8813d9a6ffdac537a184f971b4a264ea06ed5ca1ba978eeb30cac12d1d0af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
